---
title: "Assignment 2"
author: "Huy"
date: "2023-10-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# import useful library for the assignment
library(tidyverse) 
library(ggplot2)
library(readxl)
library(caret)
library(e1071)
library(glmnet)
library(ISLR)
library(car)
```

# Phase 1

Load the dataset containing monthly values of VIX and all categories of monthly EMV trackers from January 1990 - December 2022, and develop a regression model not only to for the purpose of predicting the future values of VIX. But, more importantly, the results of the regression analysis should possibly lead to new information by revealing the economic, financial and political factors that affect the behaviour of VIX over time.

For implementation purposes, you are free to either use your previous program codes, the glmnet library, or any other packages available on open source. Also, note that the last column corresponds to values of VIX at month t+1, whereas its covariates at each row pertain to month t.

Solve this problem using OLS, LASSO, Ridge Regression and Elastic Net Regression, and compare their performances by providing a thorough interpretation of their results. For each method, you may discuss its advantages and drawbacks, such as presence of multicollinearity, and then elaborate on their other aspects like feature selection, sparsity, etc.

Can you provide a coherent picture of mechanisms underlying the sudden volatility changes in stock markets in the past 23 years?

## Load The Data and Preprocess For Model Fitting

```{r}
# load the data
df <- read_excel("/Users/hwey/Downloads/EMV_VIX_Data.xlsx")

# exclude all observations with missing data
df = na.omit(df)

# return the dimensions of the data frame
dim(df)

# define the response variable 
y <- df$VIX
head(y)
dim(y)

# define matrix of predictor variables
x <- model.matrix(VIX~., data = df[,-1])[,-1]
dim(x)
```

## OLS Regression

Ordinary least square (OLS) regression is a method that allows us to find a line that best describes the relationship between one or more predictor variables and a response variable.

### Fit OLS Regression Model

```{r pressure, echo=FALSE}
# fit multiple linear regression model
ols_model <- lm(y~x, data = df)
```

### Visualize and Retract OLS Model Attributes

```{r}
# retract model statistics
summary(ols_model)

# produce added variable plots
avPlots(ols_model)
```


The coefficient of determination is 0.6209. Therefore, only 62.09% of the variation in VIX is explained by the combination of all other attributes. 


### Visualize Model Residuals

```{r pressure, echo=FALSE}
residual <- resid(ols_model)
```

```{r pressure, echo=FALSE}
plot(fitted(ols_model), residual)
abline(0,0)
```

### Analysis of OLS Regression Model

The residuals of the Multiple Linear Regression appear to be concentrating around 0 between the values of 10 and 30 of the model. The residuals, when plotted against the model, exhibits a particular pattern.

Moreover, the percentage of the variation of the response variable explainable by the predictors is low at approximately 62%.

Therefore, we failed to assume homogeneity of variances for the model and the model fails to provide a reasonable rate of accuracy when predicting the output variable. 

Hence, OLS Regression Model are not applicable.

## LASSO Regression

Previously, we failed to fit the data into a OLS Regression Model, therefore, mutlicollinearity and other factors are existent.

Lasso regression is a method we can use to fit a regression model when multicollinearity is present in the data.

In this case, we will fix the value of $\alpha$ to 1 to fit the data in Lasso Regression Model.

### Standardize Data and Fit LASSO Regression Model

```{r pressure, echo=FALSE}
# ensure reproducible results of random variables model fitting
set.seed(1)

# alpha=1 gives LASSO
lasso.cv1 = cv.glmnet(x,y, type.measure="mse", standardize=TRUE, alpha=1, family="gaussian", nlambda=200, nfolds = 10)

# most optimal lambda value
lasso.cv1$lambda.min

# maximum lambda value in the range of one std
lasso.cv1$lambda.1se

round(cbind(coef(lasso.cv1, s='lambda.min'), coef(lasso.cv1, s='lambda.1se')), digits=3)
```

### Visualize Model and Retract Related Attributes

```{r}
plot(lasso.cv1)
abline(h = lasso.cv1$cvup[lasso.cv1$index[1]], lty = 4)
```

```{r}
lasso1 = glmnet(x,y, alpha=1)
plot(lasso1, xvar='lambda')
abline(v=log(lasso.cv1$lambda.min), lty=3)
abline(v=log(lasso.cv1$lambda.1se), lty=3)
```

```{r}
predictions_lasso1 <- glmnet(x, y, alpha=1, lambda=lasso.cv$lambda.min) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE = RMSE(predictions_lasso1, y)
RMSE
```

### Fit LASSO Regression Model To Raw Data

```{r}
set.seed(1)
# fit LASSO Regression Without Data Standardization
lasso.cv2 = cv.glmnet(x,y, type.measure="mse", standardize=FALSE, alpha=1, family="gaussian", nlambda=200, nfolds=10)

lasso.cv2$lambda.min
lasso.cv2$lambda.1se

round(cbind(coef(lasso.cv2, s='lambda.min'), coef(lasso.cv2, s='lambda.1se')), digits=3)
```

### Visualize Model and Retract Related Attributes

```{r}
plot(lasso.cv2)
abline(h=lasso.cv2$cvup[lasso.cv2$index[1]], lty=4)
```

```{r}
lasso2 = glmnet(x,y,alpha=1)
plot(lasso2, xvar='lambda')
abline(v=log(lasso.cv2$lambda.min), lty=3)
abline(v=log(lasso.cv2$lambda.1se), lty=3)
```

```{r}
predictions_lasso2 <- glmnet(x, y, alpha=1, lambda=lasso.cv2$lambda.min, standardize=FALSE) %>% predict(x) %>% as.vector()

# Model performance metrics 
RMSE2 = RMSE(predictions_lasso2, y)
RMSE2
```

## Ridge Regression

In this case, we will fix the value of $\alpha$ to 0 fit the data in Ridge Regression Model and find the optimal $\lambda$ value.

### Standardize Data and Fit Ridge Regression Model

```{r}
set.seed(1)
ridge.cv1 = cv.glmnet(x, y, type.measure="mse", alpha=0, family="gaussian", nlambda=200, nfolds=10, standardize = TRUE)

# find optimal lambda value that minimizes MSE
ridge.cv1$lambda.min

# find maximum lambda value within one standard deviation
ridge.cv1$lambda.1se

# coefficient shrinkage
round(cbind(coef(ridge.cv1, s='lambda.min'), coef(ridge.cv1, s='lambda.1se')), digits=3)

```

### Visualize Model and Retrieve Related Attributes

```{r}
plot(ridge.cv1)
abline(h=ridge.cv1$cvup[ridge.cv1$index[1]], lty=4)
```

```{r}
ridge1 = glmnet(x, y, alpha = 0)
plot(ridge1, xvar = 'lambda')
abline(v = log(ridge.cv1$lambda.min), lty = 3)
abline(v = log(ridge.cv1$lambda.1se), lty = 3)
```

```{r}
predictions_ridge1 <- glmnet(x, y, alpha = 0, lambda = ridge.cv1$lambda.min) %>% predict(x) %>% as.vector()
# Model performance metrics
RMSE = RMSE(predictions_ridge1, y)
RMSE
```

### Fit Ridge Regression Model To Raw Data
```{r}
set.seed(1)
ridge.cv2 = cv.glmnet(x, y, type.measure="mse", alpha=0, family="gaussian", nlambda=200, nfolds = 10, standardize = FALSE)

# find optimal lambda value that minimizes MSE
ridge.cv2$lambda.min

# find maximum lambda value within one standard deviation
ridge.cv2$lambda.1se

# coefficient shrinkage
round(cbind(coef(ridge.cv2, s='lambda.min'), coef(ridge.cv2, s='lambda.1se')), digits=3)
```

### Visualize Model and Retrieve Related Attributes
```{r}
plot(ridge.cv2)
abline(h=ridge.cv2$cvup[ridge.cv2$index[1]], lty=4)
```

```{r}
ridge2 = glmnet(x, y, alpha=0)
plot(ridge2, xvar = 'lambda')
abline(v=log(ridge.cv2$lambda.min), lty=3)
abline(v=log(ridge.cv2$lambda.1se), lty=3)
```

```{r}
predictions_ridge2 <- glmnet(x, y, alpha=0, lambda = ridge.cv2$lambda.min, standardize = FALSE) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE2 = RMSE(predictions_ridge2, y)
RMSE2
```
## Elastic Net Regression

Use trainControl() and train() functions to fit the Elastic Net Regression model to the data. 

The goal is to find the optimal $\alpha$ and $\lambda$. 

```{r}
k <- 10
set.seed(1)
train_control <- trainControl(method = "cv", number = k)

elastic_model <- train(y ~ ., data = cbind(x, y), method = "glmnet", preProcess = c("center", "scale"), tuneLength = 10, trControl = train_control)

# print model performance metrics along with other details
print(elastic_model)
```

```{r}
print(elastic_model$finalModel$tuneValue)
```

```{r}
predictions_enet <- glmnet(x, y, alpha = elastic_model$finalModel$tuneValue$alpha, lamba = elastic_model$finalModel$tuneValue$lambda) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE_enet = RMSE(predictions_enet, y)
RMSE_enet
```

```{r}
k <- 10
set.seed(1)
train_control <- trainControl(method = "cv", number = k)

elastic_model <- train(y ~ ., data=cbind(x,y), method = "glmnet", preProcess = c("center", "scale"), tuneLength = 100, trControl = train_control)
```

```{r}
print(elastic_model$finalModel$tuneValue)
```

```{r}
predictions_enet <- glmnet(x, y, alpha = elastic_model$finalModel$tuneValue$alpha, lambda = elastic_model$finalModel$tuneValue$lambda) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE_enet = RMSE(predictions_enet, y)
RMSE_enet
```

# Phase 2

Most often, the forces driving the market movements themselves undergo changes when a transition occurs from one segment of time into its following one(s). Use elastic nets to construct predictive regression models for each segment of stock market and then interpret the results with the goal of extracting knowledge about the phenomena causing the regime switches and apparent transitions.

Can you characterize the nature of the most dominant forces responsible for driving the stock market movements within each segment? Are there any pattern on sparsity of forces when comparing chaotic (pink) and normal (blue) segments of the stock market?

Can you provide a coherent picture of mechanisms underlying the sudden volatility changes in stock markets in the past 23 years, by further considering the relatively homogeneous segments of the market?

```{r}

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
