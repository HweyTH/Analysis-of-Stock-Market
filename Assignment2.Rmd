---
title: "Assignment 2"
author: "Huy"
date: "2023-10-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# import useful library for the assignment
library(tidyverse) 
library(ggplot2)
library(readxl)
library(caret)
library(e1071)
library(glmnet)
library(ISLR)
library(car)
```

# Introduction

The stock market has always been an intriguing subject of study through the lens of data scientists and analysts. It is well-known that stock market operates in a mysterious ways which involve numerous shifts and changes. For this reason, the VIX index was generated as an estimator for such volatility. 

However, what are the considerable factors that determine the value of VIX indexes?

Considering this all-hovering question, in this report, through using Statistical Models and their Corresponding Metrics, I aim to demonstrate it is probable to forecast the stock market fear index (VIX) to an extent of accuracy and the models could be used to predict the movement of the stock market in the future.

## Load and Preprocess Data for Model Fitting

```{r}
# load the data
df <- read_excel("/Users/hwey/Downloads/EMV_VIX_Data.xlsx")
raw_data <- read_excel("/Users/hwey/Downloads/EMV_VIX_Data.xlsx")

# exclude all observations with missing data
df = na.omit(df)

# return the dimensions of the data frame
dim(df)

# define the response variable 
y <- df$VIX
head(y)
dim(y)

# define matrix of predictor variables
x <- model.matrix(VIX~., data = df[,-1])[,-1]
dim(x)

data_complete <- df[,-1]
```
In this section, the raw data of the VIX and monthly EMV trackers are loaded into df. "VIX" index entries are assigned to be response variable, while the rest of the data entries (except date variables) are collected as predictors.

The response variable is denoted by y.

The predictive variables is denoted by x. 

Upon inspecting the raw data input, it is important to note that several EMV trackers have numerous zero entries. These variables include:

- Macro - Trade EMV Tracker.

- Marco - Business Investment and Sentiment EMV Tracker.

- Exchange Rates EMV Tracker. 

- Intellectual Property Policy EMV Tracker. 

- Immigration EMV Tracker. 

- Other Regulation EMV Tracker. 

- Transportation, Infrastructure, and Public Utilities EMV Tracker. 

- Agriculural Policy EMV Tracker. 

In regards to data sparsity, the given data set does not neither empty data entries nor mismatch number of entries for the response variables and the predictors. However, the aforementioned variables, since their entries were filled with zero values, could cause model imbalance. It is essential to concern that many zero entires would shift the weight of the variables, however, since it is only 8 out of 45 variables that contain noticeable number of zero-entries, the remaining 37 variables would balance everything out. Hence, it is safe to proceed to model fitting with this data frame.  

## OLS Regression

Firstly, we can use Ordinary Least Squares Regression technique to verify the relationship between VIX and the EMV trackers. 

By assuming linearity in the relationship of VIX and the EMV metrics, we can fit the data into OLS Regression Model:

### Fit OLS Regression Model

```{r pressure, echo=FALSE}
# fit multiple linear regression model
ols_model <- lm(data_complete$VIX~., data = data_complete)

# view model summary 
summary(ols_model)
```

### Visualize OLS Regression Model

```{r}
# retrieve model statistics
summary(ols_model)

# produce added variable plots
avPlots(ols_model)
```



### Visualize OLS Regression Residuals

```{r}
residual <- resid(ols_model)
plot(fitted(ols_model), residual)
abline(0,0)
```

```{r}
# create QQ plot for the residuals
qqnorm(residual)

# add a diagonal line
qqline(residual)
```

### Interpretation of OLS Regression Model

According to the model summary, p-value is less than 2.2e-16, therefore, we can say that there is a significant relationship between VIX and other independent EMV variables. However, it is also significant to consider the Multiple R-squared metric of the model since it is 0.6214. This shows that only 62.14% of the variation in VIX values is accountable by the EMV trackers' values. Since only more than half of the variation in VIX is explainable by the EMV trackers in the model, it is safe to doubt the fitting of Ordinary Least Squared model to the data. 

Furthermore, it is important that OLS Regression Technique is performed under the assumptions of normality and homoscedasticity. 

In regards to normality, it is apparent from the Q-Q plot that the residuals of the OLS model fall roughly along the diagonal line. With insignificant data strays from the line, it can be concluded the assumption of normality is met. 

In regards to homoscedasticity, through examining the residual plot against the fitted model, the data points witness a particular pattern as they cluster in the intervals [-5, 5] on the y-axis and [10, 30] on the x axis. Thus, the residuals appear to not be randomly and evenly distributed. Hence, we fail to assume the randomness in the variances of independent variables.

In conclusion, due to the failure to assume homoscedasticity between independent variables, coupled with the relatively low Multiple R-squared value, Ordinary Least Squared Regression is not reliable in explanation of the data. 

Thereafter, we may assume the existence mutlicollinearity on the grounds of insufficient evidence in linear relationship between VIX and the EMV trackers.

```{r}
vif(ols_model)
```
Upon examining the VIF score of the EMV trackers in response to the VIX values, many of them are relatively large such as "EMV" with 148.734215, "Political Uncertainty Tracker" with 64.972093, "Macroeconomic News and Outlook EMV Tracker" with 141.731168,  
and "Macro – Broad Quantity Indicators EMV Tracker" with 13.75098. 

Since these VIF scores are considerably large, it is conclusive there exists multicollinearity between variables in our data. 

Thereafter, it is necessary to apply Lasso Regression, Ridge Regression and Net Elastic Model to deal with mutlicollinearity.

## LASSO Regression

Previously, we failed to fit OLS Regression Model to the (given) data. Additionally, we has proven multicollinearity among the predictors. Therefore, it is essential to use Least Absolute Shrinkage and Selection Operator regression to remove highly-correlated variables by shrinking their coefficients completely to zero. 

In the following sections, Lasso Regression would be applied to standardized data and raw data separately to compare the performances on the data. 

### Standardize Data and Fit LASSO Regression Model

```{r}
# ensure reproducible results of random variables model fitting
set.seed(1)

# alpha=1 gives LASSO
lasso.cv1 = cv.glmnet(x,y, type.measure="mse", standardize=TRUE, alpha=1, family="gaussian", nlambda=200, nfolds = 10)

# most optimal lambda value
lasso.cv1$lambda.min

# maximum lambda value in the range of one std
lasso.cv1$lambda.1se

round(cbind(coef(lasso.cv1, s='lambda.min'), coef(lasso.cv1, s='lambda.1se')), digits=3)
```

### Visualize Lasso Regression Model

```{r}
plot(lasso.cv1)
abline(h = lasso.cv1$cvup[lasso.cv1$index[1]], lty = 4)
```

```{r}
lasso1 = glmnet(x,y, alpha=1)
plot(lasso1, xvar='lambda')
abline(v=log(lasso.cv1$lambda.min), lty=3)
abline(v=log(lasso.cv1$lambda.1se), lty=3)
```

```{r}
predictions_lasso1 <- glmnet(x, y, alpha=1, lambda=lasso.cv1$lambda.min) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE = RMSE(predictions_lasso1, y)
RMSE
```

### Fit LASSO Regression Model To Raw Data

```{r}
set.seed(1)
# fit LASSO Regression Without Data Standardization
lasso.cv2 = cv.glmnet(x,y, type.measure="mse", standardize=FALSE, alpha=1, family="gaussian", nlambda=200, nfolds=10)

lasso.cv2$lambda.min
lasso.cv2$lambda.1se

round(cbind(coef(lasso.cv2, s='lambda.min'), coef(lasso.cv2, s='lambda.1se')), digits=3)
```

### Visualize Lasso Regression Model

```{r}
plot(lasso.cv2)
abline(h=lasso.cv2$cvup[lasso.cv2$index[1]], lty=4)
```

```{r}
lasso2 = glmnet(x,y,alpha=1)
plot(lasso2, xvar='lambda')
abline(v=log(lasso.cv2$lambda.min), lty=3)
abline(v=log(lasso.cv2$lambda.1se), lty=3)
```

```{r}
predictions_lasso2 <- glmnet(x, y, alpha=1, lambda=lasso.cv2$lambda.min, standardize=FALSE) %>% predict(x) %>% as.vector()

# Model performance metrics 
RMSE2 = RMSE(predictions_lasso2, y)
RMSE2
```

### Analysis of Lasso Regression Model

#### Standardized Data Frame 

The optimal lambda which minimizes MSE reported by the model is 0.1319708 and the lambda value which keeps cross-validation error within one standard error is 1.606589. 

16/46 variables were marked as insignificant by the model:

"Political Uncertainty Tracker"

"Macroeconomic News and Outlook EMV Tracker"

"Macro – Other Financial Indicators EMV Tracker" 

"Macro – Real Estate Markets EMV Tracker"  

"Macro – Business Investment and Sentiment EMV Tracker" 

"Financial Crises EMV Tracker"

"Exchange Rates EMV Tracker"

"Healthcare Matters EMV Tracker"

"Competition Matters EMV Tracker"

"Fiscal Policy EMV Tracker"

"Taxes EMV Tracker"

"Regulation EMV Tracker"

"Intellectual Property Policy EMV Tracker"

"Labor Regulations EMV Tracker"

"Lawsuit and Tort Reform, Supreme Court Decisions EMV Tracker"

"Housing and Land Management EMV Tracker"

"Food and Drug Policy EMV Tracker"

"Transportation, Infrastructure, and Public Utilities EMV Tracker"

"Petroleum Markets EMV Tracker"

Overall, the Root Mean Squared Error metric for the model is 4.873161.

#### Raw Data Frame

The optimal lambda which minimizes MSE reported by the model is 0.3093777 and the lambda value which keeps cross-validation error within one standard error is 2.4832.

31/45 variables were marked as insignificant by the model:
"Political Uncertainty Tracker"                           

"Infectious Disease EMV Tracker"           

"Macro – Other Financial Indicators EMV Tracker" 

"Macro – Labor Markets EMV Tracker"    

"Macro – Real Estate Markets EMV Tracker" 

"Macro – Trade EMV Tracker"      

"Macro – Business Investment and Sentiment EMV Tracker"

"Macro – Consumer Spending and Sentiment EMV Tracker" 

"Commodity Markets EMV Tracker"

"Exchange Rates EMV Tracker" 

"Healthcare Matters EMV Tracker" 

"Litigation Matters EMV Tracker" 

"Competition Matters EMV Tracker" 

"Intellectual Property Matters EMV Tracker" 

"Fiscal Policy EMV Tracker"

"Taxes EMV Tracker"

"Entitlement and Welfare Programs EMV Tracker"

"Regulation EMV Tracker"  

"Competition Policy EMV Tracker"  

"Intellectual Property Policy EMV Tracker"  

"Labor Regulations EMV Tracker" 

'Immigration EMV Tracker"  

'Energy and Environmental Regulation EMV Tracker"   

"Lawsuit and Tort Reform, Supreme Court Decisions EMV Tracker" 

"Housing and Land Management EMV Tracker"

"Other Regulation EMV Tracker"   

"Government-Sponsored Enterprises EMV Tracker"  

"Healthcare Policy EMV Tracker"

"Food and Drug Policy EMV Tracker"

"Transportation, Infrastructure, and Public Utilities EMV Tracker"

"Agricultural Policy EMV Tracker"      

"Petroleum Markets EMV Tracker"                                     
                                                                    
Overall, the Root Mean Squared Error metric for the model is 5.095759.

#### Conclusion

Upon comparing the RMSE score of the Lasso Model when applying to standardized data and raw data, we can see that the Lasso Regression Model applied to the standardized data yields lower standard deviation of the residuals, which is always better. 

Moreover, it is noticeable that when fitting Lasso Regression to raw data, 31 of the 45 independent variables were removed. Whereas, if we adjust the weight of the predictors in the data by standardizing, Lasso Regression only remove 16 of them. 

On the grounds that removing the majority of the predictive variables heavily affect the accuracy of the prediction of the model, pre-processing the data by standardization is favorable in this case. Coupled with the lower RMSE metric, 
it is conclusive that Lasso Regression Model on standardized data is reliable.

## Ridge Regression

Another way we could try to deal with multicollinearity is to apply Ridge Regression Model. Multicollinearity is resolved through the model by driving the coefficients of all predictors to zero.

In the following section, Ridge Regression would be applied to standardized and raw data and their performances are compared to obtain the better model fitting.

### Standardize Data and Fit Ridge Regression Model

```{r}
set.seed(1)
ridge.cv1 = cv.glmnet(x, y, type.measure="mse", alpha=0, family="gaussian", nlambda=200, nfolds=10, standardize = TRUE)

# find optimal lambda value that minimizes MSE
ridge.cv1$lambda.min

# find maximum lambda value within one standard deviation
ridge.cv1$lambda.1se

# coefficient shrinkage
round(cbind(coef(ridge.cv1, s='lambda.min'), coef(ridge.cv1, s='lambda.1se')), digits=3)

```

### Visualize Ridge Regression Model

```{r}
plot(ridge.cv1)
abline(h=ridge.cv1$cvup[ridge.cv1$index[1]], lty=4)
```

```{r}
ridge1 = glmnet(x, y, alpha = 0)
plot(ridge1, xvar = 'lambda')
abline(v = log(ridge.cv1$lambda.min), lty = 3)
abline(v = log(ridge.cv1$lambda.1se), lty = 3)
```

```{r}
predictions_ridge1 <- glmnet(x, y, alpha = 0, lambda = ridge.cv1$lambda.min) %>% predict(x) %>% as.vector()
# Model performance metrics
RMSE = RMSE(predictions_ridge1, y)
RMSE
```

### Fit Ridge Regression Model To Raw Data

```{r}
set.seed(1)
ridge.cv2 = cv.glmnet(x, y, type.measure="mse", alpha=0, family="gaussian", nlambda=200, nfolds = 10, standardize = FALSE)

# find optimal lambda value that minimizes MSE
ridge.cv2$lambda.min

# find maximum lambda value within one standard deviation
ridge.cv2$lambda.1se

# coefficient shrinkage
round(cbind(coef(ridge.cv2, s='lambda.min'), coef(ridge.cv2, s='lambda.1se')), digits=3)
```

### Visualize Ridge Regression Model

```{r}
plot(ridge.cv2)
abline(h=ridge.cv2$cvup[ridge.cv2$index[1]], lty=4)
```

```{r}
ridge2 = glmnet(x, y, alpha=0)
plot(ridge2, xvar = 'lambda')
abline(v=log(ridge.cv2$lambda.min), lty=3)
abline(v=log(ridge.cv2$lambda.1se), lty=3)
```

```{r}
predictions_ridge2 <- glmnet(x, y, alpha=0, lambda = ridge.cv2$lambda.min, standardize = FALSE) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE2 = RMSE(predictions_ridge2, y)
RMSE2
```

### Analysis of Ridge Regression Model

#### Standardized Data Frame

When fitting Ridge Regression Model to Standardized Data, the most optimal lambda value that minimizes MSE is 1.955834. Plus, the maximum lambda value which restricts the prediction errors within one standard deviation is 47.67237.

We witness Ridge Regression shift most the of coefficients of the predictors towards zero when $\lambda$=1.955834 is passed through the model. However, these variables display outlying behavior when their coefficients are relatively further than zero: 

"Macro – Trade EMV Tracker" -1.674

"Labor Disputes EMV Tracker" 2.041

"Intellectual Property Matters EMV Tracker" 1.596

"Immigration EMV Tracker" -2.295

"Energy and Environmental Regulation EMV Tracker" 2.739

"Other Regulation EMV Tracker" -1.492

"Agricultural Policy EMV Tracker" -4.904

Therefore, these predictors demonstrate major contribution to the response variable VIX. Additionally, the model yields a RMSE metric of 4.970971.


#### Raw Data Frame

When fitting Ridge Regression Model to Standardized Data, the most optimal lambda value that minimizes MSE is 4.585042. Plus, the maximum lambda value which restricts the prediction errors within one standard deviation is 147.5302.

We witness Ridge Regression shift all of the coefficients of the predictors towards zero when $\lambda$=4.585042 is passed through the model. Thus, the model deemed all of the predictive variables insignificant of the response variable VIX. Additionally, the model yields a RMSE metric of 5.029565.

#### Conclusion

When comparing the results from applying Ridge Regression to standardized and raw data separately, it is advisable to standardize the input data before fitting the model. Without pre-processing the data, the model fails to account for any significant relationship between the EMV trackers and VIX, which is inaccurate since we tested for multi-collinearity as above. Plus, model with standardized data generates lower RMSE value. Therefore, it is better to standardize data before fitting to Ridge Regression Model. 

## Elastic Net Regression

In previous sections, Lasso Regression Method was conducted with $\alpha$=1 and Ridge Regression Method was conducted with $\alpha$=0. However, since $\alpha$ can vary in range of 0 and 1, we can continue to build Elastic Net Regression Model and tune to the predictions to see whether we could achieve a better model with different $\alpha$ and $\lambda$ values.

The goal is to find the optimal $\alpha$ and $\lambda$ using Elastic Net Regression Method.

```{r}
k <- 10
set.seed(1)
train_control <- trainControl(method = "cv", number = k)

elastic_model <- train(y ~ ., data = cbind(x, y), method = "glmnet", preProcess = c("center", "scale"), tuneLength = 10, trControl = train_control)

# print model performance metrics along with other details
print(elastic_model)
```

```{r}
print(elastic_model$finalModel$tuneValue)
```

```{r}
predictions_enet <- glmnet(x, y, alpha = elastic_model$finalModel$tuneValue$alpha, lamba = elastic_model$finalModel$tuneValue$lambda) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE_enet = RMSE(predictions_enet, y)
RMSE_enet
```

```{r}
k <- 10
set.seed(1)
train_control <- trainControl(method = "cv", number = k)

elastic_model <- train(y ~ ., data=cbind(x,y), method = "glmnet", preProcess = c("center", "scale"), tuneLength = 100, trControl = train_control)
```

```{r}
print(elastic_model$finalModel$tuneValue)
```

```{r}
predictions_enet <- glmnet(x, y, alpha = elastic_model$finalModel$tuneValue$alpha, lambda = elastic_model$finalModel$tuneValue$lambda) %>% predict(x) %>% as.vector()

# Model performance metrics
RMSE_enet = RMSE(predictions_enet, y)
RMSE_enet
```

# Phase 2

Most often, the forces driving the market movements themselves undergo changes when a transition occurs from one segment of time into its following one(s). Use elastic nets to construct predictive regression models for each segment of stock market and then interpret the results with the goal of extracting knowledge about the phenomena causing the regime switches and apparent transitions.

Can you characterize the nature of the most dominant forces responsible for driving the stock market movements within each segment? Are there any pattern on sparsity of forces when comparing chaotic (pink) and normal (blue) segments of the stock market?

Can you provide a coherent picture of mechanisms underlying the sudden volatility changes in stock markets in the past 23 years, by further considering the relatively homogeneous segments of the market?

## Load and Split Data into Appropriate Segments

```{r}
# data subset from 1990-01 to 1998-06
segment1 = (raw_data[raw_data$Date > "1989-12" & raw_data$Date < "1998-07",])

# data subset from 1998-07 to 2003-03
segment2 = (raw_data[raw_data$Date > "1998-06" & raw_data$Date < "2003-04"])

# data subset from 2003-04 to 2007-12
segment3 = (raw_data[raw_data$Date > "2003-02" & raw_data$Date < "1998-08",])

# data subset from 2008-01 to 2009-09
segment4 = (raw_data[raw_data$Date > "2007-12" & raw_data$Date < "2009-10"])

# data subset from 2009-10 to 2019-12
segment5 = (raw_data[raw_data$Date > "2009-09" & raw_data$Date < "2020-01",])

# data subset from 2020-01 to 2022-12
segment6 = (raw_data[raw_data$Date > "2019-12" & raw_data$Date < "2023-01"])

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
